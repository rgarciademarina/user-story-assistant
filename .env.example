# LLM Configuration
MODEL_NAME="llama3.2-vision"
MODEL_TYPE="ollama"
OLLAMA_BASE_URL="http://localhost:11434"

MAX_LENGTH=2048
TEMPERATURE=0.7

# API Configuration
API_HOST="0.0.0.0"
API_PORT=8000

# Development Settings
ENVIRONMENT="development"
LOG_LEVEL="DEBUG"
DEBUG=true

# Vector Store Configuration
VECTOR_STORE_PATH="./data/vector_store"