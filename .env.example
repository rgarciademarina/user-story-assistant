# LLM Configuration
MODEL_NAME="llama3.2-vision"            # Nombre del modelo LLM a utilizar
MODEL_TYPE="ollama"                     # Tipo de modelo (e.g., ollama)
OLLAMA_BASE_URL="http://localhost:11434" # URL base para acceder a Ollama

MAX_LENGTH=2048                        # Longitud máxima de las respuestas generadas
TEMPERATURE=0.7                        # Temperatura para la generación de texto

# API Configuration
API_HOST="0.0.0.0"                      # Dirección IP en la que se ejecutará la API
API_PORT=8000                           # Puerto en el que se ejecutará la API

# Development Settings
ENVIRONMENT="development"               # Entorno de ejecución (development, production, etc.)
LOG_LEVEL="DEBUG"                       # Nivel de detalle de los logs (DEBUG, INFO, WARNING, ERROR)
DEBUG=true                              # Activar modo de depuración

# Vector Store Configuration
VECTOR_STORE_PATH="./data/vector_store" # Ruta donde se almacenarán los índices vectoriales

# Optional: Otros ajustes
# Puedes añadir más variables de entorno según las necesidades de tu proyecto
